import os
import cv2
import numpy as np
import base64
import json
import time
from pathlib import Path
from openai import OpenAI

# =================é…ç½®åŒºåŸŸ=================
# è¯·ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²è®¾ç½®ï¼Œæˆ–è€…ç›´æ¥åœ¨ä¸‹é¢å¡«å…¥å­—ç¬¦ä¸²
API_KEY = "sk-ef7db77064064747936dd65767cbd794"
# æµ‹è¯•å›¾ç‰‡è·¯å¾„ 
TEST_IMAGE_PATH = r"E:\work\oilfield-leak-detection-v4\data\original\noleak001_007408.jpg"
OUTPUT_PATH = r"E:\work\oilfield-leak-detection-v4\results\final_workflow_result.jpg"
# =========================================

# åˆå§‹åŒ– DashScope å®¢æˆ·ç«¯
client = OpenAI(
    api_key=API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)


def encode_image_to_base64(cv2_img):
    """å°† OpenCV å›¾ç‰‡è½¬æ¢ä¸º Base64 æ ¼å¼"""
    _, buffer = cv2.imencode('.jpg', cv2_img)
    return f"data:image/jpeg;base64,{base64.b64encode(buffer).decode('utf-8')}"


# ================= Stage 1: ç«¯ä¾§æµ·é€‰ (Local Proposal) =================
# å¤ç”¨ä¹‹å‰çš„å¤šå°ºåº¦+èåˆé€»è¾‘
def get_candidates_locally(image_path):
    print(f"ğŸš€ [Stage 1] æ­£åœ¨è¿›è¡Œæœ¬åœ°å¤šå°ºåº¦æµ·é€‰: {image_path}")
    full_img = cv2.imread(str(image_path))
    if full_img is None: raise FileNotFoundError(f"æ‰¾ä¸åˆ°å›¾ç‰‡: {image_path}")

    h, w = full_img.shape[:2]
    mid = w // 2

    # åˆ‡å‰²å›¾åƒ (å‡è®¾å·¦çº¢å¤–ï¼Œå³å¯è§å…‰)
    ir_part = full_img[:, :mid]
    rgb_part = full_img[:, mid:]
    ir_gray = cv2.cvtColor(ir_part, cv2.COLOR_BGR2GRAY)

    # å¤šå°ºåº¦æ£€æµ‹
    scales = [1.0, 0.5, 0.25]
    all_boxes = []

    for s in scales:
        # ç¼©æ”¾
        width = int(ir_gray.shape[1] * s)
        height = int(ir_gray.shape[0] * s)
        ir_resized = cv2.resize(ir_gray, (width, height))
        rgb_resized = cv2.resize(rgb_part, (width, height))

        # çº¢å¤–æµ
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        ir_blur = cv2.GaussianBlur(clahe.apply(ir_resized), (5, 5), 0)
        _, ir_mask = cv2.threshold(ir_blur, 200, 255, cv2.THRESH_BINARY)

        # RGBæµ
        gray = cv2.cvtColor(rgb_resized, cv2.COLOR_BGR2GRAY)
        _, rgb_mask = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV)

        # èåˆ
        if ir_mask.shape != rgb_mask.shape:
            rgb_mask = cv2.resize(rgb_mask, (ir_mask.shape[1], ir_mask.shape[0]))
        final_mask = cv2.bitwise_and(ir_mask, rgb_mask)

        # æå–æ¡†å¹¶è¿˜åŸåæ ‡
        contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for c in contours:
            if cv2.contourArea(c) > (50 * s * s):
                x, y, w_box, h_box = cv2.boundingRect(c)
                all_boxes.append([int(x / s), int(y / s), int(w_box / s), int(h_box / s)])

    # æ©è†œèåˆ (å»ç¢ç‰‡åŒ–)
    if not all_boxes: return [], rgb_part, ir_part

    canvas = np.zeros(ir_gray.shape, dtype=np.uint8)
    for (x, y, w_box, h_box) in all_boxes:
        cv2.rectangle(canvas, (x, y), (x + w_box, y + h_box), 255, -1)

    # ä½¿ç”¨ 20x20 æ ¸è¿›è¡Œèšåˆ
    kernel = np.ones((20, 20), np.uint8)
    canvas = cv2.morphologyEx(canvas, cv2.MORPH_DILATE, kernel)

    contours, _ = cv2.findContours(canvas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    final_candidates = []
    for c in contours:
        x, y, w_box, h_box = cv2.boundingRect(c)
        if w_box * h_box > 500:  # å¿½ç•¥å¾®å°å™ªç‚¹
            final_candidates.append([x, y, w_box, h_box])

    print(f"âœ… [Stage 1] æµ·é€‰å®Œæˆï¼Œå‘ç° {len(final_candidates)} ä¸ªç–‘ä¼¼ç›®æ ‡ã€‚")
    return final_candidates, rgb_part, ir_part


# ================= Stage 2: äº‘ä¾§å†³æ–­ (Cloud Reasoning) =================
def verify_with_qwen(crop_rgb, crop_ir):
    # æ‹¼å›¾ï¼šå°†çº¢å¤–å›¾(è½¬ä¸ºBGR)å’ŒRGBå›¾æ¨ªå‘æ‹¼æ¥
    if len(crop_ir.shape) == 2:
        crop_ir = cv2.cvtColor(crop_ir, cv2.COLOR_GRAY2BGR)

    # ä¸ºäº†è®©æ¨¡å‹çœ‹å¾—æ›´æ¸…æ¥šï¼Œå¦‚æœæ˜¯æå°çš„å›¾ï¼Œæ”¾å¤§ä¸€ç‚¹
    if crop_rgb.shape[0] < 64:
        crop_rgb = cv2.resize(crop_rgb, (0, 0), fx=2, fy=2)
        crop_ir = cv2.resize(crop_ir, (0, 0), fx=2, fy=2)

    combined_img = np.hstack((crop_ir, crop_rgb))
    base64_img = encode_image_to_base64(combined_img)

    prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ²¹ç”°å·¥ä¸šè§†è§‰ä¸“å®¶ã€‚å›¾ç‰‡å·¦ä¾§æ˜¯çº¢å¤–çƒ­åƒï¼ˆé«˜äº®åŒºä»£è¡¨é«˜æ¸©ï¼‰ï¼Œå³ä¾§æ˜¯å¯è§å…‰ã€‚
    è¯·åˆ¤æ–­å›¾ä¸­æ˜¯å¦åŒ…å«ã€åœ°é¢çŸ³æ²¹æ³„æ¼ã€‘ã€‚

    å¿…é¡»ä¸¥æ ¼åŒºåˆ†ï¼š
    1. ã€æ³„æ¼ (Positive)ã€‘ï¼š
       - å½¢çŠ¶ä¸è§„åˆ™ã€è¾¹ç¼˜å‘ˆé”¯é½¿çŠ¶æˆ–ç¾½åŒ–çŠ¶ã€‚
       - çœ‹èµ·æ¥åƒæ¶²ä½“æ¸—é€æˆ–æ‰©æ•£ã€‚
       - çº¢å¤–æœ‰æ¸©å·®ï¼Œä¸”å¯è§å…‰ä¸‹ä¸ºé»‘è‰²ã€‚
    2. ã€å¹²æ‰° (Negative)ã€‘ï¼š
       - ç®¡é“/è®¾å¤‡ï¼šç¬”ç›´çš„çº¿æ¡ã€è§„åˆ™çš„å‡ ä½•é•¿æ¡ã€‚
       - é˜´å½±ï¼šè¾¹ç¼˜é”åˆ©ã€å‡ ä½•å½¢çŠ¶è§„åˆ™ã€‚
       - è½¦è¾†/çŸ³å¤´ï¼šå­¤ç«‹çš„å›ºä½“å½¢æ€ã€‚

    è¯·ä»…è¾“å‡º JSONï¼š
    {"is_leak": true/false, "confidence": "high/medium/low", "reason": "ç®€è¿°åˆ¤æ–­ä¾æ®"}
    """

    try:
        completion = client.chat.completions.create(
            model="qwen2-vl-72b-instruct",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": base64_img}},
                        {"type": "text", "text": prompt},
                    ],
                }
            ],
            temperature=0.01,  # é™ä½éšæœºæ€§
        )
        content = completion.choices[0].message.content
        # ç®€å•çš„ JSON æ¸…æ´—ï¼ˆé˜²æ­¢æ¨¡å‹è¾“å‡º markdown ç¬¦å·ï¼‰
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"âš ï¸ API è°ƒç”¨å¤±è´¥: {e}")
        return {"is_leak": False, "reason": "API Error"}


# ================= ä¸»æµç¨‹ =================
def main():
    if not API_KEY:
        print("âŒ é”™è¯¯: æœªè®¾ç½® API Key")
        return

    # 1. æœ¬åœ°æµ·é€‰
    candidates, full_rgb, full_ir = get_candidates_locally(TEST_IMAGE_PATH)

    if not candidates:
        print("æœªå‘ç°ä»»ä½•ç–‘ä¼¼ç›®æ ‡ï¼Œæµç¨‹ç»“æŸã€‚")
        return

    # 2. å¾ªç¯é€å®¡
    result_img = full_rgb.copy()
    print(f"\nğŸš€ [Stage 2] å¼€å§‹äº‘ç«¯ AI å¤æ ¸ (Qwen2-VL-72B)...")

    for i, (x, y, w, h) in enumerate(candidates):
        print(f"\n--- å¤„ç†å€™é€‰åŒº {i + 1}/{len(candidates)} ---")

        # è£å‰ª (æ‰©è¾¹ 10 åƒç´ ä»¥ä¿ç•™ä¸Šä¸‹æ–‡)
        pad = 10
        x1, y1 = max(0, x - pad), max(0, y - pad)
        x2, y2 = min(full_rgb.shape[1], x + w + pad), min(full_rgb.shape[0], y + h + pad)

        crop_rgb = full_rgb[y1:y2, x1:x2]
        crop_ir = full_ir[y1:y2, x1:x2]

        # è°ƒç”¨ API
        start_time = time.time()
        result = verify_with_qwen(crop_rgb, crop_ir)
        cost_time = time.time() - start_time

        print(f"â±ï¸ è€—æ—¶: {cost_time:.2f}s")
        print(f"ğŸ¤– ç»“è®º: {result}")

        # ç»˜åˆ¶ç»“æœ
        if result.get("is_leak", False):
            # ç¡®è¯Šæ³„æ¼ï¼šç”»çº¢æ¡† + ç²—ä½“å­—
            color = (0, 0, 255)
            label = f"LEAK ({result.get('confidence', '?')})"
            thick = 3
        else:
            # æ’é™¤å¹²æ‰°ï¼šç”»ç»¿æ¡† + è™šçº¿æ•ˆæœ(æ¨¡æ‹Ÿ)
            color = (0, 255, 0)
            label = "Ignored"
            thick = 2

        cv2.rectangle(result_img, (x, y), (x + w, y + h), color, thick)
        cv2.putText(result_img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # 3. ä¿å­˜æœ€ç»ˆç»“æœ
    Path(OUTPUT_PATH).parent.mkdir(exist_ok=True, parents=True)
    cv2.imwrite(OUTPUT_PATH, result_img)
    print(f"\nâœ… å…¨æµç¨‹ç»“æŸï¼")
    print(f"ğŸ“‚ ç»“æœå›¾ç‰‡å·²ä¿å­˜: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()